{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importazione delle librerie necessarie\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.font_manager as fm\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.models import Model\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caricamento del modello DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caricamento del modello RESNET50\n",
    "model = torch.hub.load('facebookresearch/detr:main', 'detr_resnet50', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
    "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
    "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
    "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
    "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
    "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
    "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
    "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
    "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
    "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
    "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "    'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per convertire x, y, w e h in (x1, y1) e (x2, y2)\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "\n",
    "    return torch.stack(b, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per riscalare le bounding boxes\n",
    "def rescale_bboxes(boxes, size):\n",
    "    \n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(boxes)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detect pedestrians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per rilevare i pedoni (modifica di quella della prof)\n",
    "def detect_pedestrians(threshold_confidence, model, im, transform = None):\n",
    "    if transform is None:\n",
    "\n",
    "        # standard PyTorch mean-std input image normalization\n",
    "        transform = T.Compose([\n",
    "        T.Resize(800),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    img = transform(im).unsqueeze(0)\n",
    "\n",
    "    # demo model only support by default images with aspect ratio between 0.5 and 2\n",
    "    # if you want to use images with an aspect ratio outside this range\n",
    "    # rescale your image so that the maximum size is at most 1333 for best results\n",
    "    assert img.shape[-2] <= 1600 and img.shape[-1] <= 1600, 'demo model only supports images up to 1600 pixels on each side'\n",
    "\n",
    "    outputs = model(img)\n",
    "\n",
    "    # keep only predictions with a confidence > threshold_confidence\n",
    "    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "    max_probas = probas.max(-1).values\n",
    "    keep = probas.max(-1).values > threshold_confidence\n",
    "    labels = probas.argmax(-1)\n",
    "\n",
    "    # Filter by pedestrian\n",
    "    keep = keep & (labels == 1)\n",
    "\n",
    "    # Extract the confidences for the kept boxes\n",
    "    confidences = max_probas[keep].detach().numpy()\n",
    "\n",
    "    # convert boxes from [0; 1] to image scales\n",
    "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
    "\n",
    "    return confidences, bboxes_scaled.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estrazione delle detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_detections(folder_path, t):\n",
    "    frame_files = sorted([os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(('.png', '.jpg', '.jpeg'))])\n",
    "    frame_count = 0\n",
    "\n",
    "    detections_preload = []\n",
    "\n",
    "    idx = 0\n",
    "    # preload frames\n",
    "    for frame_file in frame_files:\n",
    "        frame = Image.open(frame_file)\n",
    "        print(f'Frame: {idx}')\n",
    "        confidences, detections = detect_pedestrians(t, im=frame, model=model)\n",
    "\n",
    "        detection_per_frame = []\n",
    "        for i in range(len(detections)):\n",
    "            detection_per_frame.append([detections[i], confidences[i]])\n",
    "        detections_preload.append(detection_per_frame)\n",
    "        idx += 1\n",
    "\n",
    "    return detections_preload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prova sul campo (soglia confidence a 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\Users\\caste\\Unipa - IntelliCrafters\\Visione artificiale\\Assignments\\Assignment 3\\dataset\\train\\MOT17-13-DPM\\img1'\n",
    "detections = extract_detections(path, t=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvataggio delle detections su file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('\"C:\\Users\\caste\\Unipa - IntelliCrafters\\Visione artificiale\\Assignments\\Assignment 3\\outputs\"\\MOT17-13-DPM-DETR06.txt', 'w') as f:\n",
    "    frame_count = 0\n",
    "    for detections_frame in detections:\n",
    "        # indice del frame, detections e confidences\n",
    "        for i in range(len(detections_frame)):\n",
    "            \n",
    "            detections_str = \", \".join([str(value) for value in detections_frame[i][0]])\n",
    "\n",
    "            print(f'{frame_count}, {detections_str}, {detections_frame[i][1]}', file=f)\n",
    "        frame_count += 1\n",
    "        # for conf, detect in zip(confidences, detections):\n",
    "        #     print(f'{detect},{conf}', file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prova sul campo (soglia confidence a 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'dataset/train/MOT17-02-DPM/img1'\n",
    "#detections = extract_detections(path, t=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvataggio delle detections su file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''with open('outputs/MOT17-02-DPM/MOT17-02-DPM-DETR06.txt', 'w') as f:\n",
    "    frame_count = 0\n",
    "    for detections_frame in detections:\n",
    "        # indice del frame, detections e confidences\n",
    "        for i in range(len(detections_frame)):\n",
    "            \n",
    "            detections_str = \", \".join([value for value in detections_frame[i][0]])\n",
    "\n",
    "            print(f'{frame_count}, {detections_str}, {detections_frame[i][1]}', file=f)\n",
    "        frame_count += 1\n",
    "        # for conf, detect in zip(confidences, detections):\n",
    "        #     print(f'{detect},{conf}', file=f)'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
